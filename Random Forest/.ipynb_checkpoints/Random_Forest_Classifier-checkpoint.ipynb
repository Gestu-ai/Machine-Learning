{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qoOwWA8Jomxl"
   },
   "source": [
    "# Random Forest Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksmMsvVLoclX"
   },
   "source": [
    "Random forests are known as ensemble learning methods used for classification and regression, but in this particular case I'll be focusing on classification. Random forests are essentially a collection of decision trees that are each fit on a subsample of the data. While an individual tree is typically noisey and subject to high variance, random forests average many different trees, which in turn reduces the variability and leave us with a powerful classifier.\n",
    "\n",
    "Random forests are also non-parametric and require little to no parameter tuning. They differ from many common machine learning models used today that are typically optimized using gradient descent. Models like linear regression, support vector machines, neural networks, etc. require a lot of matrix based operations, while tree based models like random forest are constructed with basic arithmetic. In other words, to build a tree all we're really doing is selecting a hand full of observations from out dataset, picking a few features to look through, and finding the value that makes the best split in our data. We'll define what what it means to be the \"best split\" in a bit.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6s-rFfVg-gbT"
   },
   "outputs": [],
   "source": [
    "class RandomForest():\n",
    "    def __init__ (self, x, y, n_trees, sample_sz=None, min_leaf=5):\n",
    "        np.random.seed(42) \n",
    "        if sample_sz is None:\n",
    "            sample_sz=len(y)\n",
    "        self.x, self.y, self.sample_sz, self.min_leaf = x, y, sample_sz, min_leaf\n",
    "        self.trees = [self.create_tree() for i in range(n_trees)]\n",
    "    \n",
    "    def create_tree(self):\n",
    "        idxs = np.random.choice(len(self.y), replace=True, size = self.sample_sz)      \n",
    "        return DecisionTree(self.x.iloc[idxs], \n",
    "                            self.y[idxs],\n",
    "                            idxs=np.array(range(self.sample_sz)),\n",
    "                            min_leaf=self.min_leaf)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        percents = np.mean([t.predict(x) for t in self.trees], axis=0)\n",
    "        return [1 if p>0.5 else 0 for p in percents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GUFhVY8H-rm-"
   },
   "outputs": [],
   "source": [
    "\n",
    "def find_gini(left, right, y):\n",
    "    classes  = np.unique(y)\n",
    "    n = len(left) + len(right)\n",
    "    s1=0; s2=0\n",
    "    \n",
    "    for k in classes:   \n",
    "        p1 = len(np.nonzero(y[left] == k)[0]) / len(left)\n",
    "        s1 += p1*p1 \n",
    "        p2 = len(np.nonzero(y[right] == k)[0]) / len(right)\n",
    "        s2 += p2*p2 \n",
    "    \n",
    "    gini = (1-s1)*(len(left)/n) + (1-s2)*(len(right)/n)\n",
    "    \n",
    "    return gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JA7t6JX-rpy"
   },
   "outputs": [],
   "source": [
    "class DecisionTree():\n",
    "    def __init__(self, x, y, idxs=None, min_leaf=5):  \n",
    "        if idxs is None: \n",
    "            idxs=np.arange(len(y))\n",
    "        self.x, self.y, self.idxs, self.min_leaf = x, y, idxs, min_leaf   \n",
    "        self.n, self.c = len(idxs), x.shape[1]\n",
    "        self.val = np.mean(y[idxs])\n",
    "        self.score = float('inf')\n",
    "        self.find_varsplit()\n",
    "        \n",
    "    @property\n",
    "    def split_name(self): return self.x.columns[self.var_idx]\n",
    "    \n",
    "    @property\n",
    "    def split_cols(self): return self.x.values[self.idxs, self.var_idx]\n",
    "    \n",
    "    @property\n",
    "    def is_leaf(self): return self.score == float('inf')\n",
    "    \n",
    "    def __repr__(self):\n",
    "        s = f'n: {self.n}'\n",
    "        if not self.is_leaf:\n",
    "            s+= f'; gini:{self.score}; split:{self.split}; var: {self.split_name}'\n",
    "        return s\n",
    "            \n",
    "    def check_features(self):\n",
    "        for i in range(self.c): \n",
    "            self.find_better_split(i)\n",
    "    \n",
    "    def find_best_split(self, var_idx):\n",
    "    \n",
    "        x, y = self.x.values[self.idxs, var_idx], self.y[self.idxs]   \n",
    "        sort_idx = np.argsort(x)\n",
    "        sort_y = y[sort_idx]\n",
    "        sort_x = x[sort_idx]\n",
    "\n",
    "        for i in range(0, self.n-self.min_leaf-1):\n",
    "            if i < self.min_leaf or sort_x[i] == sort_x[i+1]: continue \n",
    "            lhs = np.nonzero(sort_x <= sort_x[i])[0]\n",
    "            rhs = np.nonzero(sort_x > sort_x[i])[0]\n",
    "            if rhs.sum()==0: continue\n",
    "\n",
    "            gini = find_gini(lhs, rhs, sort_y)\n",
    "\n",
    "            if gini<self.score: \n",
    "                self.var_idx, self.score, self.split = var_idx, gini, sort_x[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m97X4WIF-rr_"
   },
   "outputs": [],
   "source": [
    "tree = TreeEnsemble(x_sub, y_train, 1).trees[0]\n",
    "tree"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Random Forest Classifier.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
