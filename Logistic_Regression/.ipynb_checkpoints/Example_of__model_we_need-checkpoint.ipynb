{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uIZHFfA0cStM"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Gestu-ai/Machine-Learning/blob/master/Logistic_Regression/Example_of__model_we_need.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q1WnrEJHcTyz"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/Gestu-ai/Machine-Learning/master/images-master/gestu1.png?token=AIW35UAKBBELC5SYDUECSYC7HQ4FW\" width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vx9rHF2gcStN"
   },
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_5paLW1qcStO"
   },
   "source": [
    "* # [Part I - Description](#part1)\n",
    "\n",
    "* # [Part II - Algorithm](#part2)\n",
    "    1. [Hypothesis](#part2)\n",
    "    2. [Cost function](#part2)\n",
    "    3. [Updating parameters process](#interactive-plot)\n",
    "* # [Part III - Implementation](#part3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1CHnM9CncStP"
   },
   "source": [
    "***\n",
    "## Part I - Description  <a name=\"part1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4K0hLor_cStQ"
   },
   "source": [
    "* Logistic regression is one of the most popular Machine Learning algorithms, which comes under the Supervised Learning technique. It is used for predicting the categorical dependent variable using a given set of independent variables.\n",
    "\n",
    "* Logistic regression predicts the output of a categorical dependent variable. Therefore the outcome must be a categorical or discrete value. It can be either Yes or No, 0 or 1, true or False, etc. but instead of giving the exact value as 0 and 1, it gives the probabilistic values which lie between 0 and 1.\n",
    "\n",
    "* Logistic Regression is much similar to the Linear Regression except that how they are used. Linear Regression is used for solving Regression problems, whereas Logistic regression is used for solving the classification problems.\n",
    "* In Logistic regression, instead of fitting a regression line, we fit an \"S\" shaped logistic function, which predicts two maximum values (0 or 1).\n",
    "The curve from the logistic function indicates the likelihood of something such as whether the cells are cancerous or not, a mouse is obese or not based on its weight, etc.\n",
    "* Logistic Regression is a significant machine learning algorithm because it has the ability to provide probabilities and classify new data using continuous and discrete datasets.\n",
    "* Logistic Regression can be used to classify the observations using different types of data and can easily determine the most effective variables used for the classification. The below image is showing the logistic function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gawVSl3ocStR"
   },
   "source": [
    "***\n",
    "## Part II - Algorithm  <a name=\"part1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dUw_0Dh1cStR"
   },
   "source": [
    "### 1. Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OuQRpB56cStS"
   },
   "source": [
    "**The hypothesis of logistic regression is defined by** $\\sigma(x w+b) =\\frac{1}{1 + e^{-(xw+b)}}$.\n",
    "\n",
    "The function $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ which is called logistic function or sigmoid function.\n",
    "\n",
    "For the following steps we use the notation $z = x w+b$, $\\hat{y}=\\sigma(z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d6lfdz-ELNGU"
   },
   "source": [
    "#####bold text The code for hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlZIJwoKKWDC"
   },
   "outputs": [],
   "source": [
    "def sigmoid(self , z):\n",
    "  \n",
    "  self.sigmo= 1/(1+np.exp(-z))\n",
    "  \n",
    "  return self.sigmo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LuU5F5NVcStT"
   },
   "source": [
    "### 2. Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GPDxOmfccStU"
   },
   "source": [
    "In this we are going to use MLE(Maximum Likelihood Eestimation) to define the cost function of logistic regression model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1xeFrcizcStX"
   },
   "source": [
    "Consequently, by using Bernoulli distribution we can write \n",
    "\n",
    "$$% \\begin{center}\n",
    "\\\\\n",
    "\\begin{align*} \n",
    "P(Y=y)&=\\hat{y}^{y}(1-\\hat{y}^{1-y})\\\\\n",
    "&=\\sigma(xw+b)^{y}(1-\\sigma(xw+b)^{1-y})\n",
    "\\end{align*}\n",
    "%\\end{center}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v1dLAy0ZcStY"
   },
   "source": [
    "And now considering all the data set we can write the likelihood by:\n",
    "    \n",
    "$$\n",
    "\\begin{align*}\n",
    "l(w,b)& =\\prod_{i=1}^{n}P(Y=y^{(i)} | X=x^{(i)})\\\\\n",
    "&=\\prod_{i=1}^{n}\\sigma(x^{(i)} w + b)^{y^{(i)}}+(1-\\sigma(x^{(i)} w+b))^{1-y^{(i)}}\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m7qKEnrZcStZ"
   },
   "source": [
    "As we know that the logarithm function is monotonic, the parameter which maximise the likelihood is the same parameter which maximize the logarithm of the likelihood.\n",
    "\n",
    "$$ log(l(w, b))=\\sum_{i=1}^{n}y^{(i)}log\\sigma(x^{(i)} w+b) + (1-y^{(i)})\\log(1-\\sigma(x^{(i)} w+b))$$\n",
    "\n",
    "Our objective is to find the parameter which maximize the term $log(l(w,b))$ but we nkow also that the procedure of maximization of a function $f$ can reformulate as a procedure of minimization of $-f$.\n",
    "\n",
    "$ argmax_{w,b} \\sum_{i=1}^{n}y^{(i)}log\\sigma(x^{(i)} w+b) + (1-y^{(i)})\\log(1-\\sigma(x^{(i)} w+b))  = argmin_{w,b} [ - \\sum_{i=1}^{n}y^{(i)}log\\sigma(x^{(i)} w+b) + (1-y^{(i)})\\log(1-\\sigma(x^{(i)} w+b))]$.\n",
    "\n",
    "As our objective becomes now to minimize the function $- \\sum_{i=1}^{n}y^{(i)}log\\sigma(x^{(i)} w+b) + (1-y^{(i)})\\log(1-\\sigma(x^{(i)} w+b))$. \n",
    "\n",
    "\n",
    "Finally the cost function of logistic regression is given by: \n",
    "$\\begin{align*}\n",
    "L(w,b)=- log(l(w,b))&=-\\sum_{i=1}^{n}y^{(i)}log\\sigma(x^{(i)}w+b) + (1-y^{(i)})\\log(1-\\sigma(x^{(i)}w+b))\\\\\n",
    "&=-\\sum_{i=1}^{n}y^{(i)}log(\\hat{y}^{i}) + (1-y^{(i)})\\log(1-\\hat{y}^{i})\n",
    "\\end{align*}\n",
    "$ \n",
    "\n",
    "$L(w,b)$  is called **negative log likelihood**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1uYyNM4BLgMd"
   },
   "source": [
    "##### The code for the cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GRwqpI3TLtwC"
   },
   "outputs": [],
   "source": [
    "def cost_function (self, x, y, y_true):\n",
    "  \n",
    "  self.cost = -(1/ x.shape[0]) * np.sum(y_true * np.log(y) + (1-y_true) * np.log(1- y))\n",
    "    \n",
    "  return self.cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H8OIWSOHcSta"
   },
   "source": [
    "### 3. Updating parameters "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UuPIfoalcSta"
   },
   "source": [
    "To update the parameter $w$(respectively $b$) which is a vector, we are going to simplify it by updating component by component. So, to update $w_j$ (respectively $b$), we start by computing the gradient of $L$ with respect to $w_j$ (respectively $b$):\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial}{\\partial w_{j}}L(w,b) & = \\frac{\\partial}{\\partial \\hat{y}^{i}}L(w,b)*\\frac{\\partial}{\\partial w_{j}}\\hat{y}^{i}\\\\\n",
    "&=-\\frac{y}{\\hat{y}^{i}}+\\frac{(1-y)}{(1-\\hat{y}^{i})}\\\\\n",
    "&= \\frac{\\hat{y}^{i}-y}{\\hat{y}^{i}(1-\\hat{y}^{i})}\\\\\n",
    "\\end{align*}\n",
    "\\\\\n",
    "\\frac{\\partial}{\\partial w_{j}}\\hat{y}^{i} = \\hat{y}^{i}(1-\\hat{y}^{i})*x_{j}^{i}\n",
    "$$\n",
    "\n",
    "consequently $$\\frac{\\partial}{\\partial w_{j}}L(w,b)= (\\hat{y}^{i}-y)x_{j}^{i}$$\n",
    "\n",
    "And $$\\frac{\\partial}{\\partial w}L(w,b)=\\frac{1}{n}\\sum_{i=1}^{n}(\\hat{y}^{i}-y)x_{j}^{i}$$\n",
    "\n",
    "By analogy, the derivative of $L(w,b)$ with respect to $b$ is given by:\n",
    "\n",
    "$$\\frac{\\partial}{\\partial b}L(w,b)=\\frac{1}{n}\\sum_{i=1}^{n}(\\hat{y}^{i}-y)$$\n",
    "\n",
    "To update the parameters $w$ and $b$ by considering a learning rate $\\lambda$, for each iteration, we do the following instruction: $$\\begin{align*} w_{current} &= w_{previous} - \\lambda * \\frac{\\partial}{\\partial w}L(w,b) \\\\\n",
    "b_{current} &= b_{previous} - \\lambda * \\frac{\\partial}{\\partial b}L(w,b)\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XnFHT5o9MYYu"
   },
   "source": [
    "##### The code for updating the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vzkir_K-L974"
   },
   "outputs": [],
   "source": [
    "def learning_algorithm(x, y_true, learning_rate, n_iteration) :\n",
    "  m_samples, n_features = x.shape\n",
    "  theta = np.zeros((n_features , 1))\n",
    "  bias = 0\n",
    "  cost_final= np.zeros(n_iteration)\n",
    "  \n",
    "  for i in range(n_iteration):\n",
    "    y_hat = sigmoid(np.dot(x, theta) + bias)\n",
    "\n",
    "    cost = cost_function(x, y_hat, y_true)\n",
    "    \n",
    "    d_theta = (1 / m_samples) * np.dot(x.T, (y_hat - y_true))\n",
    "    d_bias = (1 / m_samples) * np.sum(y_hat - y_true)\n",
    "\n",
    "    theta = theta - learning_rate * d_theta \n",
    "    bias = bias - learning_rate * d_bias\n",
    "\n",
    "    cost_final[i] = cost\n",
    "        \n",
    "        \n",
    "  return theta, bias, cost_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gA8ihADoMiaS"
   },
   "source": [
    "## The learning algorithm of logistic regression model into an Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "s0zVZ1QZcStb",
    "outputId": "1510c240-d7b9-4bd8-b0c8-37c845faf2f4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sn\n",
    "import random as rd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CCsxNY0OcStg"
   },
   "outputs": [],
   "source": [
    "X = pd.read_csv('X_train.csv')\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MlVYnW97cStk",
    "outputId": "d64f8934-f7d7-4178-d5f5-828edbbe51f0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y\n",
       "0  1\n",
       "1  1\n",
       "2  1\n",
       "3  1\n",
       "4  0"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.read_csv('y_train.csv')\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dkva-DDfcStp"
   },
   "source": [
    "## here, we convert the target data frame to an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FPakc-x5cStq"
   },
   "outputs": [],
   "source": [
    "X=X.values\n",
    "y=y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Py22az_cStu"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ySd8IBxLcStx",
    "outputId": "03559917-4fd9-495e-8581-19f19191d9fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(660, 6)\n",
      "(166, 6)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FW_IqHjMcSt2"
   },
   "outputs": [],
   "source": [
    "class LogisticRegression ():\n",
    "    \n",
    "    def __init__(self) :\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def sigmoid(self , z):\n",
    "        \n",
    "        self.sigmo= 1/(1+np.exp(-z))\n",
    "        \n",
    "        return self.sigmo\n",
    "    \n",
    "    def cost_function (self, x, y, y_true):\n",
    "       \n",
    "\n",
    "        self.cost = -(1/ x.shape[0]) * np.sum(y_true * np.log(y) + (1-y_true) * np.log(1- y))\n",
    "        \n",
    "        return self.cost\n",
    "    \n",
    "    def learning_algorithm(self, x, y_true, learning_rate, n_iteration) :\n",
    "        \n",
    "        m_samples, n_features = x.shape\n",
    "        self.theta = np.zeros((n_features , 1))\n",
    "        self.bias = 0\n",
    "        cost_final= np.zeros(n_iteration)\n",
    "        \n",
    "        for i in range(n_iteration):\n",
    "            \n",
    "            y_hat = self.sigmoid(np.dot(x, self.theta) + self.bias)\n",
    "\n",
    "            cost = self.cost_function(x, y_hat, y_true)\n",
    "            \n",
    "            d_theta = (1 / m_samples) * np.dot(x.T, (y_hat - y_true))\n",
    "            d_bias = (1 / m_samples) * np.sum(y_hat - y_true)\n",
    "\n",
    "            self.theta = self.theta - learning_rate * d_theta \n",
    "            self.bias = self.bias - learning_rate * d_bias\n",
    "        \n",
    "            cost_final[i] = cost\n",
    "            \n",
    "            \n",
    "        return self.theta, self.bias, cost_final\n",
    "        \n",
    "        \n",
    "    def prediction (self, X):\n",
    "        \n",
    "        y_hat = self.sigmoid ( np.dot(X, self.theta) + self.bias)\n",
    "        \n",
    "        y_predict_labels = [1 if elem > 0.5 else 0 for elem in y_hat]\n",
    "        \n",
    "        \n",
    "        return y_predict_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AMe5UlXKcSt6"
   },
   "outputs": [],
   "source": [
    "logistic_reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H4OgQ1mNcSt9"
   },
   "source": [
    "### Here we wanna show what the curve looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E731ips8cSt-",
    "outputId": "a2292620-0dbf-4590-c862-b3a124d82d1d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "logistic_reg = LogisticRegression()\n",
    "theta, bias, costs = logistic_reg.learning_algorithm(X_train, y_train, learning_rate=0.1, n_iteration=1000)\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "plt.plot(np.arange (1000), costs)\n",
    "plt.title(\"Development of cost over training\")\n",
    "plt.xlabel(\"Number of iterations\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vD3zV2uzcSuB"
   },
   "source": [
    "### We can check that this curve is decreasing but it is stationnary somewhere when the number of iteration becomes very large. That's the convergence of the cost function.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RzIctj4ncSuC"
   },
   "outputs": [],
   "source": [
    "gr_descent=logistic_reg.learning_algorithm(X_train, y_train, learning_rate=0.1, n_iteration=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9nviCqqGcSuG"
   },
   "outputs": [],
   "source": [
    "def checking_accuracy(y_predicted, y):\n",
    "    accuracy=0\n",
    "    for i in range(len(y_predicted)):\n",
    "        if y_predicted[i]==y[i]:\n",
    "            accuracy+= 1\n",
    "\n",
    "    accuracy/= len(y_predicted)\n",
    "    \n",
    "    return accuracy\n",
    "#     print('the accuracy on the training set is {} %'.format(np.round(accuracy_train*100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Dv5xEVycSuJ"
   },
   "source": [
    "### Accuracy on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LLChd5mRcSuK",
    "outputId": "894f44c0-be5f-4139-9d17-4564c273e16c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy on the test set is 92.12 %\n"
     ]
    }
   ],
   "source": [
    "y_predict_train=logistic_reg.prediction(X_train)\n",
    "\n",
    "accuracy_train = checking_accuracy(y_predict_train, y_train)\n",
    "print('the accuracy on the test set is {} %'.format(np.round(accuracy_train*100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MT6R8HiucSuN"
   },
   "source": [
    "### Accuracy on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OX8duRescSuO",
    "outputId": "2489d842-a173-4a7d-c1b5-db5da8090ce6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the accuracy on the test set is 88.55 %\n"
     ]
    }
   ],
   "source": [
    "y_predict_test=logistic_reg.prediction(X_test)\n",
    "\n",
    "accuracy_test = checking_accuracy(y_predict_test, y_test)\n",
    "print('the accuracy on the test set is {} %'.format(np.round(accuracy_test*100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-njWSDzfcSuR"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KEk6B7FOcSuS"
   },
   "source": [
    "As we have seen that the difference between the accuracy on the training set and the accuracy on the testing set is less than 4%, so we can have confidence that \n",
    "our model was learning very well. It didn't overfit"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Example_of _model_we_need.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
