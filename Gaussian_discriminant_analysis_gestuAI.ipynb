{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "from IPython.display import Latex"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%latex \n",
    "from IPython.display import Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Discriminant Analysis (GDA)\n",
    "GDA is part of the algorithms called **Generative learning algorthm**. We call the so because of the fact that they try to model $p(x/y)$ and $p(y)$.\n",
    "After modeling $p(y)$ (called the **class priors**) and $p(x|y)$, the  algorithm can they use Bayes rule to derive the posterior distribution on $y$ given:\n",
    "$$p(y|x) = \\frac{p(x|y) p(y)}{p(x)}$$\n",
    "Here in case of binary classification, the denominator is given by $p(x) = p(x|y=1)p(y=1) + p(x|y=0)p(y=0)$. Actually, if we were calculating $p(y|x)$ in order to make a prediction, then we don't actually need to calculate the denominator, since $$\\begin{aligned}\n",
    "\\underset{y}{\\arg \\max } p(y \\mid x) &=\\arg \\max _{y} \\frac{p(x \\mid y) p(y)}{p(x)} \\\\\n",
    "&=\\arg \\max _{y} p(x \\mid y) p(y)\n",
    "\\end{aligned}$$\n",
    "The particularity of the GDA model is the fact that we will assume that $p(x|y)$ is distributed according to a multivariate normal distribution.\n",
    "\n",
    "When we have a classification problem in which the input features $x$ are continuous-valued random variables, we can then use the Gaussian Discriminant Analysis (GDA) model, which models p(x|y) using a multivariate normal distribution. The model is:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "y & \\sim \\operatorname{Bernoulli}(\\phi) \\\\\n",
    "x \\mid y=0 & \\sim \\mathcal{N}\\left(\\mu_{0}, \\Sigma\\right) \\\\\n",
    "x \\mid y=1 & \\sim \\mathcal{N}\\left(\\mu_{1}, \\Sigma\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "Writing out the distributions, this is:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(y) &=\\phi^{y}(1-\\phi)^{1-y} \\\\\n",
    "p(x \\mid y=0) &=\\frac{1}{(2 \\pi)^{n / 2}|\\Sigma|^{1 / 2}} \\exp \\left(-\\frac{1}{2}\\left(x-\\mu_{0}\\right)^{T} \\Sigma^{-1}\\left(x-\\mu_{0}\\right)\\right) \\\\\n",
    "p(x \\mid y=1) &=\\frac{1}{(2 \\pi)^{n / 2}|\\Sigma|^{1 / 2}} \\exp \\left(-\\frac{1}{2}\\left(x-\\mu_{1}\\right)^{T} \\Sigma^{-1}\\left(x-\\mu_{1}\\right)\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "Here, the parameters of our model are $\\phi, \\Sigma, \\mu_{0}$ and $\\mu_{1}$. (Note that while there're two different mean vectors $\\mu_{0}$ and $\\mu_{1},$ this model is usually applied using only one covariance matrix $\\Sigma$. ) The log-likelihood of the data is given by\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ell\\left(\\phi, \\mu_{0}, \\mu_{1}, \\Sigma\\right) &=\\log \\prod_{i=1}^{m} p\\left(x^{(i)}, y^{(i)} ; \\phi, \\mu_{0}, \\mu_{1}, \\Sigma\\right) \\\\\n",
    "&=\\log \\prod_{i=1}^{m} p\\left(x^{(i)} \\mid y^{(i)} ; \\mu_{0}, \\mu_{1}, \\Sigma\\right) p\\left(y^{(i)} ; \\phi\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "By maximizing the likelihood $l$ with respect to the parameters, we find the maximum likelihood from estimate of the parameters to be: \n",
    "\\begin{aligned}\n",
    "\\phi &=\\frac{1}{m} \\sum_{i=1}^{m} 1\\left\\{y^{(i)}=1\\right\\} \\\\\n",
    "\\mu_{0} &=\\frac{\\sum_{i=1}^{m} 1\\left\\{y^{(i)}=0\\right\\} x^{(i)}}{\\sum_{i=1}^{m} 1\\left\\{y^{(i)}=0\\right\\}} \\\\\n",
    "\\mu_{1} &=\\frac{\\sum_{i=1}^{m} 1\\left\\{y^{(i)}=1\\right\\} x^{(i)}}{\\sum_{i=1}^{m} 1\\left\\{y^{(i)}=1\\right\\}} \\\\\n",
    "\\Sigma &=\\frac{1}{m} \\sum_{i=1}^{m}\\left(x^{(i)}-\\mu_{y^{(i)}}\\right)\\left(x^{(i)}-\\mu_{y^{(i)}}\\right)^{T}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the GDA from scratch\n",
    "In this part we are going the to implementation step by step the estimation of the parameters and use them them to compute the maximum likelihood.\n",
    "We will finish it by combine all the process into an Object that we can call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the prior probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 0.33333333, 0.33333333])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def prior_probability(target):\n",
    "    theta = np.array([(target==c).mean() for c in np.unique(target)])\n",
    "    return theta\n",
    "\n",
    "prior_probability(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the estimation of the expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.006, 3.428, 1.462, 0.246],\n",
       "       [5.936, 2.77 , 4.26 , 1.326],\n",
       "       [6.588, 2.974, 5.552, 2.026]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def estimated_means(X,target):\n",
    "    mean = []\n",
    "    for c in np.unique(target):\n",
    "        dc = X[target==c]\n",
    "        mc = dc.mean(axis = 0)\n",
    "        mean.append(mc)\n",
    "    mu = np.array(mean)\n",
    "    return mu\n",
    "\n",
    "estimated_means(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the covariance matrice of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.259708  , 0.09086667, 0.164164  , 0.03763333],\n",
       "       [0.09086667, 0.11308   , 0.05413867, 0.032056  ],\n",
       "       [0.164164  , 0.05413867, 0.181484  , 0.041812  ],\n",
       "       [0.03763333, 0.032056  , 0.041812  , 0.041044  ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cov_matrix(X,target):\n",
    "    Data = X.copy()\n",
    "    m = estimated_means(X,target)\n",
    "    for v in np.unique(target):\n",
    "        Data[target==v] = Data[target==v] - m[v]\n",
    "    cov = np.dot(Data.T,Data)/len(y)\n",
    "    inv_cov = np.linalg.pinv(cov)\n",
    "    cov = cov\n",
    "    return cov\n",
    "\n",
    "cov_matrix(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GDAnalysis(object):\n",
    "    \n",
    "    def __init__(self,theta = None, mu = None, cov = None,prediction = None):\n",
    "        self.theat = theta\n",
    "        self.mu = mu\n",
    "        self.cov = cov\n",
    "        self.prediction = prediction\n",
    "        \n",
    "    def prior_probability(self,target):\n",
    "        self.theta = np.array([(target==c).mean() for c in np.unique(target)])\n",
    "        return self.theta\n",
    "\n",
    "    def estimated_means(self,X,target):\n",
    "        mean = []\n",
    "        for c in np.unique(target):\n",
    "            dc = X[target==c]\n",
    "            mc = dc.mean(axis = 0)\n",
    "            mean.append(mc)\n",
    "        self.mu = np.array(mean)\n",
    "        return self.mu\n",
    "\n",
    "    def cov_matrix(self,X,target):\n",
    "        Data = X.copy()\n",
    "        m = self.estimated_means(X,target)\n",
    "        for v in np.unique(target):\n",
    "            Data[target==v] = Data[target==v] - m[v]\n",
    "        cov = np.dot(Data.T,Data)/len(target)\n",
    "        self.inv_cov = np.linalg.pinv(cov)\n",
    "        self.cov = cov\n",
    "        return cov\n",
    "    \n",
    "    \n",
    "    def gauss_density(self,x):\n",
    "        \"\"\"pdf of the multivariate normal distribution.\"\"\"\n",
    "        x_m = x - self.mu\n",
    "        return np.exp(-1 * np.sum(x_m.dot(self.inv_cov) * x_m, axis=1)) * 0.5 * self.theta\n",
    "    \n",
    "    def fit(self,X,target):\n",
    "        self.prior_probability(target) \n",
    "        self.estimated_means(X,target)\n",
    "        self.cov_matrix(X,target)\n",
    "        return self.theta , self.mu , self.cov\n",
    "   \n",
    "    def predict(self,X):\n",
    "        self.prediction = np.apply_along_axis(self.gauss_density, 1, X)\n",
    "        return np.argmax(self.prediction, axis=1)\n",
    "    \n",
    "    def accuracy(self, y_test,y_pred):\n",
    "        return (y_test==y_pred).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.33333333, 0.33333333, 0.33333333])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GDA = GDAnalysis()\n",
    "GDA.prior_probability(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.006, 3.428, 1.462, 0.246],\n",
       "       [5.936, 2.77 , 4.26 , 1.326],\n",
       "       [6.588, 2.974, 5.552, 2.026]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GDA.estimated_means(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.259708  , 0.09086667, 0.164164  , 0.03763333],\n",
       "       [0.09086667, 0.11308   , 0.05413867, 0.032056  ],\n",
       "       [0.164164  , 0.05413867, 0.181484  , 0.041812  ],\n",
       "       [0.03763333, 0.032056  , 0.041812  , 0.041044  ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GDA.cov_matrix(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11.06481504, -5.48691938, -9.17543051,  3.4871337 ],\n",
       "       [-5.48691938, 14.52436643,  2.7242858 , -9.08804849],\n",
       "       [-9.17543051,  2.7242858 , 15.09102422, -9.08813896],\n",
       "       [ 3.4871337 , -9.08804849, -9.08813896, 37.52283607]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GDA.inv_cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre = GDA.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GDA.accuracy(y, pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
